# -*- coding: utf-8 -*-
"""Colab Obesitas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l5OWZsZIuOKziD0GuIV33zU-kH8Js2J7

**Rendra Gunawan//A11.2022.14235//DS05**
Link google Colab: https://colab.research.google.com/drive/1l5OWZsZIuOKziD0GuIV33zU-kH8Js2J7?usp=sharing
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score

# membaca dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/Bengkod/ObesityDataSet.csv'
df = pd.read_csv(file_path, na_values='?')
df.head(10)

"""# EDA"""

# Menampilkan informasi dataset
print("--- Informasi Dataset ---")
df.info()

# menampilkan deskripsi dataset numerik
print("--- Deskripsi Statistik Dataset Numerik ---")
df.describe()

# menampilkan deskripsi dataset kategorikal
print("--- Deskripsi Statistik Dataset Kategorikal---")
df.describe(include=['object','category'])

# mendeteksi missing value
missing = df.isna().sum() #missing = missing value
print("--- Missing values per column ---\n", missing)

# menampilkan data unik
print("--- Data Unik --- \n", df.nunique())

# mendeteksi duplikat
print("Jumlah data duplikat:", df.duplicated().sum())

# menentukan keseimbangan
target_counts = df['NObeyesdad'].value_counts()
print("--- Target Balance --- \n", target_counts)

# boxplot untuk mendeteksi outlier variabel kontinu
numeric_columns = ["Age", "Height", "Weight", "CH2O", "FAF", "TUE"]
valid_continuous = [col for col in numeric_columns if col in df.columns]
plt.figure(figsize=(18, 12))
for i, col in enumerate(valid_continuous,1):
    plt.subplot(3, 3, i)
    sns.boxplot(data=df, x=col, orient='h')
    plt.title(f'Boxplot: {col}')
    plt.tight_layout()

plt.suptitle('Boxplots untuk Deteksi Outlier variabel kontinu', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# menentukan histogram fitur kontinu
valid_cols = [col for col in numeric_columns if col in df.columns]
for col in valid_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

plt.figure(figsize=(12, 8))
if valid_cols:
    df[valid_cols].hist(bins=15, figsize=(12, 8))
    plt.suptitle('Histogram Fitur Kontinu')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# Target balance
plt.figure(figsize=(6,4))
sns.countplot(x='NObeyesdad',data=df)
plt.xticks(rotation=45)
plt.show()
print(df['NObeyesdad'].value_counts(normalize=True))

# Heatmap korelasi fitur kontinu
plt.figure(figsize=(8, 6))
if valid_cols:
    corr_matrix = df[valid_cols].corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
    plt.title('Heatmap Korelasi Fitur Kontinu')
    plt.tight_layout()
    plt.show()

"""# Kesimpulan EDA

Pada tahap EDA memiliki permasalahan yaitu banyak kolom numerik yang terbaca sebagai objek, untuk data yang hilang sedikit, dan ada beberapa data duplikat. tapi distribusi kelas pada variabel target teridentifikasi relatif seimbang, faktor kelas pada variabel target ini merupakan kondisi ideal untuk pemodelan klasifikasi

# Preprocessing
"""

# memisahkan fitur(X) dan target(y)
target_col = 'NObeyesdad'
X = df.drop(columns=[target_col])
y = df[target_col].copy()
print(f"Shapes â€“ X: {X.shape}, y: {y.shape}")

# Identifikasi kolom numerik dan kategorikal
numerik_cols = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']
kategori_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']

# Konversi kolom numerik menjadi float
for col in numerik_cols:
    X[col] = pd.to_numeric(X[col], errors='coerce')

# mengecek apakah tipe data kolom numerik sudah menjadi float
print("--- Info Tipe kolom Numerik ---")
X[numerik_cols].info()

# menangani missing value pada kolom numerik dan kategori
imputer_num = SimpleImputer(strategy='median')
imputer_cat = SimpleImputer(strategy='most_frequent')

X[numerik_cols] = imputer_num.fit_transform(X[numerik_cols])
X[kategori_cols] = imputer_cat.fit_transform(X[kategori_cols])
print("Jumlah missing values: ", pd.DataFrame(X).isnull().sum().sum())

# One-hot encode untuk kolom kategorikal
X = pd.get_dummies(X, columns=kategori_cols, drop_first=True)
print(f"\nBentuk data X setelah one-hot encoding: {X.shape}")

# Standarisasi fitur numerik
scaler = StandardScaler()
X[numerik_cols] = scaler.fit_transform(X[numerik_cols])

print("\n=== Baris Pertama Data X Setelah Preprocessing Selesai ===")
display(X.head())

# df_clean = pd.concat([X, y.reset_index(drop=True)], axis=1)
# df_clean.to_csv('/content/drive/MyDrive/Colab Notebooks/Bengkod/ObesityDataSet_cleaned.csv', index=False)
# dataset diatas berhasil disimpan pada google drive sehingga command dijadikan pagar agar tidak tersimpan berkali-kali

#Visualisasi distribusi target
y.value_counts().plot(kind='bar', color='skyblue')
plt.title('Distribusi Kategori Target (NObeyesdad)')
plt.xlabel('Kategori')
plt.ylabel('Jumlah')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#Hitung korelasi antara fitur dan target
encoded_target = y.astype('category').cat.codes
df_corr_all = X[numerik_cols].copy()
df_corr_all['target_encoded'] = encoded_target

corr_target = df_corr_all.corr(numeric_only=True)['target_encoded'].drop('target_encoded')

# Visualisasi korelasi terhadap target
plt.figure(figsize=(8, 4))
sns.barplot(x=corr_target.values, y=corr_target.index)
plt.title('Korelasi Fitur Numerik terhadap Target')
plt.xlabel('Korelasi')
plt.ylabel('Fitur')
plt.tight_layout()
plt.show()

"""# Kesimpulan Preprocessing

Di tahap ini tipe data yang memiliki kesalahan dikonversi menjadi numerik, nilai kosong diisi dengan strategi median dan modus, lalu dilakukan one-hot encoding, dan standarisasi fitur numerik.

# Modeling
"""

# membagi data menjadi data train, dan data test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"data train(X_train): {X_train.shape}") #ukuran data yang dilatih
print(f"data test(X_test): {X_test.shape}") #ukuran data yang diuji

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy

    print(f"{name} Accuracy: {accuracy:.4f}")
    print(classification_report(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot(cmap=plt.cm.Blues, values_format='d', xticks_rotation=45)
    plt.title(f'Confusion Matrix - {name} (Sebelum Tunning)')
    plt.show()

"""# Kesimpulan Modeling

Pada proses modeling menggunakan Logistic Regression, Random Forest, dan K-Nearest Neighbors (KNN) bertujuan untuk mendapatkan performa awal. Hasil yang diperoleh menunjukkan bila Random Forest secara signifikan lebih unggul dengan akurasi awal mencapai 93%

# Hyperparameter tuning
"""

# Grid untuk Logistic Regression dan KNN (menggunakan GridSearchCV)
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs']
}
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'p': [1, 2] # 1: Jarak Manhattan, 2: Jarak Euclidean
}

# Distribusi parameter untuk Random Forest (menggunakan RandomizedSearchCV)
param_dist_rf = {
    'n_estimators': [100, 150, 200, 250, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

best_models = {}
tuned_accuracies = {}

# Inisialisasi pencarian parameter
searches = {
    'Logistic Regression': GridSearchCV(models['Logistic Regression'], param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1),
    'Random Forest': RandomizedSearchCV(models['Random Forest'], param_dist_rf, n_iter=50, cv=5, scoring='accuracy', n_jobs=-1, random_state=42),
    'K-Nearest Neighbors': GridSearchCV(models['K-Nearest Neighbors'], param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)
}

for name, search in searches.items():
    print(f"--- Tuning Hyperparameter untuk {name} ---")
    search.fit(X_train, y_train)

    print(f"Parameter Terbaik: {search.best_params_}")
    print(f"Skor Akurasi Terbaik dari Cross-Validation: {search.best_score_:.4f}")

    # Simpan model terbaik
    best_model = search.best_estimator_
    best_models[name] = best_model

    # Evaluasi pada data uji
    y_pred_best = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred_best)
    tuned_accuracies[name] = accuracy

    print("Laporan Klasifikasi di Test Set (Setelah Tuning):")
    print(f"Akurasi: {accuracy:.4f}")
    print(classification_report(y_test, y_pred_best, zero_division=0))

    # Confusion Matrix untuk model yang sudah dituning
    cm = confusion_matrix(y_test, y_pred_best, labels=best_model.classes_)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
    disp.plot(cmap=plt.cm.Blues, values_format='d', xticks_rotation=45)
    plt.title(f'Confusion Matrix: {name} (Setelah Tuning)')
    plt.tight_layout()
    plt.show()

df_compare = pd.DataFrame({
    'Akurasi Sebelum Tuning': results,
    'Akurasi Setelah Tuning': tuned_accuracies
}).sort_values(by='Akurasi Setelah Tuning', ascending=False)

print("--- Tabel Perbandingan Performa Model ---")
display(df_compare)

# Menentukan model terbaik
best_model_name = df_compare.index[0]
best_accuracy = df_compare.iloc[0, 1]
print(f"Model terbaik setelah tuning adalah '{best_model_name}' dengan akurasi {best_accuracy:.4f} pada data uji.")

df_compare.plot(kind='bar', figsize=(12, 7))
plt.title('Perbandingan Akurasi Model Sebelum dan Sesudah Tuning', fontsize=16)
plt.ylabel('Akurasi', fontsize=12)
plt.xlabel('Model', fontsize=12)
plt.xticks(rotation=0)
plt.ylim(0.7, 1.0) # Fokus pada rentang akurasi yang relevan
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(loc='lower right')

# Menambahkan label nilai di atas setiap bar
for index, row in enumerate(df_compare.iterrows()):
    plt.text(index - 0.2, row[1]['Akurasi Sebelum Tuning'] + 0.005, f"{row[1]['Akurasi Sebelum Tuning']:.3f}", color='black')
    plt.text(index + 0.05, row[1]['Akurasi Setelah Tuning'] + 0.005, f"{row[1]['Akurasi Setelah Tuning']:.3f}", color='black', fontweight='bold')

plt.tight_layout()
plt.show()

"""# Kesimpulan Hyperparameter

Pada tahap ini dilakukan hyperparameter tuning menggunakan GridSearchCV untuk 2 model yaitu Logistic Regression & KKN, serta RandomizedSearchCv untuk Random Forest. hasilnya model Random Forest memiliki hasil terbaik yaitu 94% kemudian KNN dengan hasil 86% dan terakhir Logistic Regression dengan akurasi 73%. ini membuktikan bahwa Random Forest merupakan model yang paling akurat dan cocok untuk kasus obeistas.

**Menyimpan Model**
"""

# Menyimpan model terbaik (Random Forest)
#joblib.dump(best_models['Random Forest'], '/content/drive/MyDrive/Colab Notebooks/Bengkod/random_forest_obesity_model.pkl')

# Menyimpan objek scaler yang telah di-fit pada data latih
#joblib.dump(scaler, '/content/drive/MyDrive/Colab Notebooks/Bengkod/scaler.pkl')

#print("Model 'random_forest_obesity_model.pkl' dan scaler 'scaler.pkl' berhasil disimpan!")